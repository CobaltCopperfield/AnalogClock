{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1872400,"sourceType":"datasetVersion","datasetId":1112583},{"sourceId":8270457,"sourceType":"datasetVersion","datasetId":4910262},{"sourceId":8279817,"sourceType":"datasetVersion","datasetId":4917032}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==2.15.0\n!pip install numpy==1.24.0","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:49:06.696560Z","iopub.execute_input":"2024-07-02T10:49:06.696818Z","iopub.status.idle":"2024-07-02T10:49:38.771860Z","shell.execute_reply.started":"2024-07-02T10:49:06.696794Z","shell.execute_reply":"2024-07-02T10:49:38.770765Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow==2.15.0 in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.51.1)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15.0) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.2.1\n    Uninstalling keras-3.2.1:\n      Successfully uninstalled keras-3.2.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\nCollecting numpy==1.24.0\n  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nDownloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nalbumentations 1.4.0 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\nfeaturetools 1.30.0 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.24.0 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nseaborn 0.12.2 requires numpy!=1.24.0,>=1.17, but you have numpy 1.24.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorstore 0.1.56 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nwoodwork 0.30.0 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\nxarray 2024.3.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.24.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:49:38.773796Z","iopub.execute_input":"2024-07-02T10:49:38.774103Z","iopub.status.idle":"2024-07-02T10:49:50.597037Z","shell.execute_reply.started":"2024-07-02T10:49:38.774075Z","shell.execute_reply":"2024-07-02T10:49:50.596246Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-02 10:49:40.593758: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-02 10:49:40.593900: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-02 10:49:40.713573: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the number of classes (e.g., 12 hours * 60 minutes)\nnum_classes = 12 * 60","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:49:50.598181Z","iopub.execute_input":"2024-07-02T10:49:50.598693Z","iopub.status.idle":"2024-07-02T10:49:50.602874Z","shell.execute_reply.started":"2024-07-02T10:49:50.598667Z","shell.execute_reply":"2024-07-02T10:49:50.601848Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess dataset\ndataset_path = '/kaggle/input/simple-analog-clock-monochrome/clock_minute_hand/samples/'\nfilenames = os.listdir(dataset_path)\n\n# Process labels to remove image extensions and convert them to one-hot encoded format\nlabels = {}\nfor filename in filenames:\n    # Remove the '.png' extension and split by '_'\n    time_parts = filename.replace('.png', '').split('_')\n    # Convert to numerical format (e.g., minutes past midnight)\n    hour = int(time_parts[1])\n    if hour == 12:\n        hour = 0  # Set 12 to 0\n    minute = int(time_parts[2])\n    total_minutes = hour * 60 + minute\n    # One-hot encode the label\n    one_hot_label = to_categorical(total_minutes, num_classes)\n    labels[filename] = one_hot_label\n\n# Read images and convert labels\nimages = []\nprocessed_labels = []\nfor filename in filenames:\n    # Read and preprocess the image\n    img = cv2.imread(os.path.join(dataset_path, filename), cv2.IMREAD_GRAYSCALE)\n    img = cv2.resize(img, (224, 224))  # Resize to match the input shape of the model\n    img = img / 255.0  # Normalize pixel values\n    images.append(img)\n    \n    # Process and store the label\n    processed_labels.append(labels[filename])\n\n# Convert lists to numpy arrays\nimages = np.array(images).reshape(-1, 224, 224, 1)  # Add the channel dimension for grayscale\nprocessed_labels = np.array(processed_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:49:50.604215Z","iopub.execute_input":"2024-07-02T10:49:50.604571Z","iopub.status.idle":"2024-07-02T10:50:42.594248Z","shell.execute_reply.started":"2024-07-02T10:49:50.604539Z","shell.execute_reply":"2024-07-02T10:50:42.593264Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"processed_labels","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:50:42.597045Z","iopub.execute_input":"2024-07-02T10:50:42.597368Z","iopub.status.idle":"2024-07-02T10:50:42.605649Z","shell.execute_reply.started":"2024-07-02T10:50:42.597341Z","shell.execute_reply":"2024-07-02T10:50:42.604753Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# Model architecture\ninputs = Input(shape=(224, 224, 1))\n\nx = Conv2D(filters = 256, kernel_size = (3,3), padding = 'valid', activation = 'relu')(inputs)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size = (2,2), padding = \"valid\")(x)\n\n\nx = Conv2D(filters = 256, kernel_size = (3,3), padding = 'valid', activation = 'relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size = (2,2), padding = \"valid\")(x)\n\n\nx = Conv2D(filters = 128, kernel_size = (3,3), padding = 'valid', activation = 'relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size = (2,2), padding = \"valid\")(x)\n\n\nx = Conv2D(filters = 128, kernel_size = (3,3), padding = 'valid', activation = 'relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size = (2,2), padding = \"valid\")(x)\n\n\nx = Conv2D(filters = 256, kernel_size = (3,3), padding = 'valid', activation = 'relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size = (2,2), padding = \"valid\")(x)\n\n\nx = Flatten()(x)\n\nx = Dense(units = 128, activation = 'relu')(x)\nx = Dropout(0.1)(x)\n\nx = Dense(units = 128, activation = 'relu')(x)\nx = Dropout(0.1)(x)\n\nx = Dense(units = 64, activation = 'relu')(x)\nx = Dropout(0.1)(x)\n\nx = Dense(units = 32, activation = 'relu')(x)\nx = Dropout(0.1)(x)\n\n# Adjust the last Dense layer to match the number of classes\nx = Dense(units=num_classes, activation='softmax')(x)\n\n# Create the model\nmodel = Model(inputs=inputs, outputs=x)\n\n# Compile and train the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:50:42.606767Z","iopub.execute_input":"2024-07-02T10:50:42.607040Z","iopub.status.idle":"2024-07-02T10:50:43.256227Z","shell.execute_reply.started":"2024-07-02T10:50:42.607016Z","shell.execute_reply":"2024-07-02T10:50:43.255428Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:50:43.257374Z","iopub.execute_input":"2024-07-02T10:50:43.257657Z","iopub.status.idle":"2024-07-02T10:50:43.316999Z","shell.execute_reply.started":"2024-07-02T10:50:43.257633Z","shell.execute_reply":"2024-07-02T10:50:43.314022Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 222, 222, 256)     2560      \n                                                                 \n batch_normalization (Batch  (None, 222, 222, 256)     1024      \n Normalization)                                                  \n                                                                 \n max_pooling2d (MaxPooling2  (None, 111, 111, 256)     0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 109, 109, 256)     590080    \n                                                                 \n batch_normalization_1 (Bat  (None, 109, 109, 256)     1024      \n chNormalization)                                                \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 54, 54, 256)       0         \n g2D)                                                            \n                                                                 \n conv2d_2 (Conv2D)           (None, 52, 52, 128)       295040    \n                                                                 \n batch_normalization_2 (Bat  (None, 52, 52, 128)       512       \n chNormalization)                                                \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n g2D)                                                            \n                                                                 \n conv2d_3 (Conv2D)           (None, 24, 24, 128)       147584    \n                                                                 \n batch_normalization_3 (Bat  (None, 24, 24, 128)       512       \n chNormalization)                                                \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 12, 12, 128)       0         \n g2D)                                                            \n                                                                 \n conv2d_4 (Conv2D)           (None, 10, 10, 256)       295168    \n                                                                 \n batch_normalization_4 (Bat  (None, 10, 10, 256)       1024      \n chNormalization)                                                \n                                                                 \n max_pooling2d_4 (MaxPoolin  (None, 5, 5, 256)         0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 6400)              0         \n                                                                 \n dense (Dense)               (None, 128)               819328    \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 128)               16512     \n                                                                 \n dropout_1 (Dropout)         (None, 128)               0         \n                                                                 \n dense_2 (Dense)             (None, 64)                8256      \n                                                                 \n dropout_2 (Dropout)         (None, 64)                0         \n                                                                 \n dense_3 (Dense)             (None, 32)                2080      \n                                                                 \n dropout_3 (Dropout)         (None, 32)                0         \n                                                                 \n dense_4 (Dense)             (None, 720)               23760     \n                                                                 \n=================================================================\nTotal params: 2204464 (8.41 MB)\nTrainable params: 2202416 (8.40 MB)\nNon-trainable params: 2048 (8.00 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"AnalogClockTimeRecModel.h5\"\ncheckpoint = ModelCheckpoint(model_name, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n\ncalls = EarlyStopping(patience = 5, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:50:43.322964Z","iopub.execute_input":"2024-07-02T10:50:43.323218Z","iopub.status.idle":"2024-07-02T10:50:43.329995Z","shell.execute_reply.started":"2024-07-02T10:50:43.323196Z","shell.execute_reply":"2024-07-02T10:50:43.329130Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"history = model.fit(images, processed_labels, epochs = 100, validation_split = 0.2, callbacks = [checkpoint, calls], verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:50:43.331291Z","iopub.execute_input":"2024-07-02T10:50:43.332039Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1719917456.045854     113 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"250/250 [==============================] - ETA: 0s - loss: 6.2632 - accuracy: 0.0075\nEpoch 1: val_loss improved from inf to 6.53222, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 68s 211ms/step - loss: 6.2632 - accuracy: 0.0075 - val_loss: 6.5322 - val_accuracy: 0.0015\nEpoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"250/250 [==============================] - ETA: 0s - loss: 4.7453 - accuracy: 0.0457\nEpoch 2: val_loss improved from 6.53222 to 4.18268, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 4.7453 - accuracy: 0.0457 - val_loss: 4.1827 - val_accuracy: 0.1125\nEpoch 3/100\n250/250 [==============================] - ETA: 0s - loss: 3.9199 - accuracy: 0.0854\nEpoch 3: val_loss improved from 4.18268 to 2.92665, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 3.9199 - accuracy: 0.0854 - val_loss: 2.9266 - val_accuracy: 0.2115\nEpoch 4/100\n250/250 [==============================] - ETA: 0s - loss: 3.4489 - accuracy: 0.1310\nEpoch 4: val_loss improved from 2.92665 to 2.46100, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 3.4489 - accuracy: 0.1310 - val_loss: 2.4610 - val_accuracy: 0.2760\nEpoch 5/100\n250/250 [==============================] - ETA: 0s - loss: 3.1279 - accuracy: 0.1761\nEpoch 5: val_loss improved from 2.46100 to 2.23321, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 3.1279 - accuracy: 0.1761 - val_loss: 2.2332 - val_accuracy: 0.3200\nEpoch 6/100\n250/250 [==============================] - ETA: 0s - loss: 2.8410 - accuracy: 0.2077\nEpoch 6: val_loss improved from 2.23321 to 1.81773, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 2.8410 - accuracy: 0.2077 - val_loss: 1.8177 - val_accuracy: 0.4215\nEpoch 7/100\n250/250 [==============================] - ETA: 0s - loss: 2.6294 - accuracy: 0.2418\nEpoch 7: val_loss improved from 1.81773 to 1.74041, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 2.6294 - accuracy: 0.2418 - val_loss: 1.7404 - val_accuracy: 0.4230\nEpoch 8/100\n250/250 [==============================] - ETA: 0s - loss: 2.4892 - accuracy: 0.2720\nEpoch 8: val_loss improved from 1.74041 to 1.54370, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 2.4892 - accuracy: 0.2720 - val_loss: 1.5437 - val_accuracy: 0.4885\nEpoch 9/100\n250/250 [==============================] - ETA: 0s - loss: 2.3429 - accuracy: 0.2961\nEpoch 9: val_loss improved from 1.54370 to 1.41226, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 2.3429 - accuracy: 0.2961 - val_loss: 1.4123 - val_accuracy: 0.5520\nEpoch 10/100\n250/250 [==============================] - ETA: 0s - loss: 2.1825 - accuracy: 0.3307\nEpoch 10: val_loss improved from 1.41226 to 1.27315, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 2.1825 - accuracy: 0.3307 - val_loss: 1.2732 - val_accuracy: 0.5895\nEpoch 11/100\n250/250 [==============================] - ETA: 0s - loss: 2.1641 - accuracy: 0.3402\nEpoch 11: val_loss improved from 1.27315 to 1.17782, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 2.1641 - accuracy: 0.3402 - val_loss: 1.1778 - val_accuracy: 0.6495\nEpoch 12/100\n250/250 [==============================] - ETA: 0s - loss: 2.0605 - accuracy: 0.3616\nEpoch 12: val_loss improved from 1.17782 to 1.12287, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 2.0605 - accuracy: 0.3616 - val_loss: 1.1229 - val_accuracy: 0.6435\nEpoch 13/100\n250/250 [==============================] - ETA: 0s - loss: 1.9920 - accuracy: 0.3839\nEpoch 13: val_loss improved from 1.12287 to 1.10531, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 201ms/step - loss: 1.9920 - accuracy: 0.3839 - val_loss: 1.1053 - val_accuracy: 0.6730\nEpoch 14/100\n250/250 [==============================] - ETA: 0s - loss: 1.9502 - accuracy: 0.4030\nEpoch 14: val_loss improved from 1.10531 to 0.98068, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 201ms/step - loss: 1.9502 - accuracy: 0.4030 - val_loss: 0.9807 - val_accuracy: 0.6980\nEpoch 15/100\n250/250 [==============================] - ETA: 0s - loss: 1.8605 - accuracy: 0.4145\nEpoch 15: val_loss did not improve from 0.98068\n250/250 [==============================] - 50s 200ms/step - loss: 1.8605 - accuracy: 0.4145 - val_loss: 1.0338 - val_accuracy: 0.6640\nEpoch 16/100\n250/250 [==============================] - ETA: 0s - loss: 1.8290 - accuracy: 0.4280\nEpoch 16: val_loss improved from 0.98068 to 0.87673, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 1.8290 - accuracy: 0.4280 - val_loss: 0.8767 - val_accuracy: 0.7505\nEpoch 17/100\n250/250 [==============================] - ETA: 0s - loss: 1.7627 - accuracy: 0.4512\nEpoch 17: val_loss improved from 0.87673 to 0.84700, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 1.7627 - accuracy: 0.4512 - val_loss: 0.8470 - val_accuracy: 0.8005\nEpoch 18/100\n250/250 [==============================] - ETA: 0s - loss: 1.7379 - accuracy: 0.4624\nEpoch 18: val_loss improved from 0.84700 to 0.81140, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 1.7379 - accuracy: 0.4624 - val_loss: 0.8114 - val_accuracy: 0.7795\nEpoch 19/100\n250/250 [==============================] - ETA: 0s - loss: 1.6806 - accuracy: 0.4834\nEpoch 19: val_loss improved from 0.81140 to 0.71090, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 1.6806 - accuracy: 0.4834 - val_loss: 0.7109 - val_accuracy: 0.7980\nEpoch 20/100\n250/250 [==============================] - ETA: 0s - loss: 1.6489 - accuracy: 0.4799\nEpoch 20: val_loss did not improve from 0.71090\n250/250 [==============================] - 50s 201ms/step - loss: 1.6489 - accuracy: 0.4799 - val_loss: 0.7426 - val_accuracy: 0.7785\nEpoch 21/100\n250/250 [==============================] - ETA: 0s - loss: 1.6208 - accuracy: 0.5056\nEpoch 21: val_loss improved from 0.71090 to 0.66664, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 1.6208 - accuracy: 0.5056 - val_loss: 0.6666 - val_accuracy: 0.8295\nEpoch 22/100\n250/250 [==============================] - ETA: 0s - loss: 1.5734 - accuracy: 0.5194\nEpoch 22: val_loss did not improve from 0.66664\n250/250 [==============================] - 50s 201ms/step - loss: 1.5734 - accuracy: 0.5194 - val_loss: 0.7470 - val_accuracy: 0.7835\nEpoch 23/100\n250/250 [==============================] - ETA: 0s - loss: 1.5799 - accuracy: 0.5073\nEpoch 23: val_loss did not improve from 0.66664\n250/250 [==============================] - 50s 201ms/step - loss: 1.5799 - accuracy: 0.5073 - val_loss: 0.6988 - val_accuracy: 0.8245\nEpoch 24/100\n250/250 [==============================] - ETA: 0s - loss: 1.4912 - accuracy: 0.5345\nEpoch 24: val_loss did not improve from 0.66664\n250/250 [==============================] - 50s 201ms/step - loss: 1.4912 - accuracy: 0.5345 - val_loss: 0.8248 - val_accuracy: 0.7660\nEpoch 25/100\n250/250 [==============================] - ETA: 0s - loss: 1.4897 - accuracy: 0.5405\nEpoch 25: val_loss did not improve from 0.66664\n250/250 [==============================] - 50s 201ms/step - loss: 1.4897 - accuracy: 0.5405 - val_loss: 0.6894 - val_accuracy: 0.8340\nEpoch 26/100\n250/250 [==============================] - ETA: 0s - loss: 1.4450 - accuracy: 0.5631\nEpoch 26: val_loss improved from 0.66664 to 0.54234, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 202ms/step - loss: 1.4450 - accuracy: 0.5631 - val_loss: 0.5423 - val_accuracy: 0.8740\nEpoch 27/100\n250/250 [==============================] - ETA: 0s - loss: 1.4376 - accuracy: 0.5626\nEpoch 27: val_loss improved from 0.54234 to 0.45363, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 201ms/step - loss: 1.4376 - accuracy: 0.5626 - val_loss: 0.4536 - val_accuracy: 0.9000\nEpoch 28/100\n250/250 [==============================] - ETA: 0s - loss: 1.4208 - accuracy: 0.5761\nEpoch 28: val_loss did not improve from 0.45363\n250/250 [==============================] - 50s 201ms/step - loss: 1.4208 - accuracy: 0.5761 - val_loss: 0.5163 - val_accuracy: 0.8980\nEpoch 29/100\n250/250 [==============================] - ETA: 0s - loss: 1.4123 - accuracy: 0.5757\nEpoch 29: val_loss did not improve from 0.45363\n250/250 [==============================] - 50s 200ms/step - loss: 1.4123 - accuracy: 0.5757 - val_loss: 0.4658 - val_accuracy: 0.9080\nEpoch 30/100\n250/250 [==============================] - ETA: 0s - loss: 1.3861 - accuracy: 0.5842\nEpoch 30: val_loss improved from 0.45363 to 0.41871, saving model to AnalogClockTimeRecModel.h5\n250/250 [==============================] - 50s 200ms/step - loss: 1.3861 - accuracy: 0.5842 - val_loss: 0.4187 - val_accuracy: 0.9190\nEpoch 31/100\n250/250 [==============================] - ETA: 0s - loss: 1.3631 - accuracy: 0.5910\nEpoch 31: val_loss did not improve from 0.41871\n250/250 [==============================] - 50s 200ms/step - loss: 1.3631 - accuracy: 0.5910 - val_loss: 0.4690 - val_accuracy: 0.9270\nEpoch 32/100\n250/250 [==============================] - ETA: 0s - loss: 1.3486 - accuracy: 0.5953\nEpoch 32: val_loss did not improve from 0.41871\n250/250 [==============================] - 50s 201ms/step - loss: 1.3486 - accuracy: 0.5953 - val_loss: 0.4224 - val_accuracy: 0.9385\nEpoch 33/100\n250/250 [==============================] - ETA: 0s - loss: 1.3229 - accuracy: 0.6059\nEpoch 33: val_loss did not improve from 0.41871\n250/250 [==============================] - 50s 201ms/step - loss: 1.3229 - accuracy: 0.6059 - val_loss: 0.4957 - val_accuracy: 0.9090\nEpoch 34/100\n124/250 [=============>................] - ETA: 23s - loss: 1.2900 - accuracy: 0.6227","output_type":"stream"}]},{"cell_type":"code","source":"# Plot training history\nplt.figure(figsize=(12, 6))\n\n# Plot training and validation loss\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Manual Test","metadata":{}},{"cell_type":"code","source":"# Function to preprocess the image\ndef preprocess_image(image_path, img_height=224, img_width=224):\n    # Read and preprocess the image\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    img = cv2.resize(img, (img_height, img_width))  # Resize to match the input shape of the model\n    img = img / 255.0  # Normalize pixel values\n    img = np.array(img).reshape(-1, img_height, img_width, 1)  # Add the channel dimension for grayscale\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the trained model\nmodel = load_model('AnalogClockTimeRecModel.keras')\n\n# Path to your new image\nnew_image_path = r'/kaggle/input/clockface-handwritten/fc3.png'\n\n# Preprocess the image\npreprocessed_image = preprocess_image(new_image_path)\n\n# Get the prediction\nprediction = model.predict(preprocessed_image)\n\n# Convert the prediction to the original label\npredicted_minutes = np.argmax(prediction)\npredicted_hour = predicted_minutes // 60\npredicted_minute = predicted_minutes % 60\n\n# Adjust the hour if it was set to 0 for 12 o'clock\nif predicted_hour == 0:\n    predicted_hour = 12\n\nprint(f\"The predicted time is {predicted_hour}:{predicted_minute:02d}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the prediction\nprediction = model.predict(preprocessed_image)\n\n# Get the indices of the top 10 predictions\ntop_10_indices = np.argsort(prediction[0])[-10:][::-1]\n\n# Get the top 10 probabilities\ntop_10_probabilities = prediction[0][top_10_indices]\n\n# Convert the indices to the original labels (time)\ntop_10_predictions = [(index // 60, index % 60) for index in top_10_indices]\n\n# Adjust the hours if they were set to 0 for 12 o'clock\ntop_10_predictions = [(12 if hour == 0 else hour, minute) for hour, minute in top_10_predictions]\n\n# Print the top 10 predictions with their probabilities\nfor i, (hour, minute) in enumerate(top_10_predictions):\n    print(f\"Prediction {i+1}: {hour}:{minute:02d} with probability {top_10_probabilities[i]:.10f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}